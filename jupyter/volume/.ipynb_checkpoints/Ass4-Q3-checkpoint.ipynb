{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02633f4b-acda-4631-adbb-f9851faa5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05ea8e24-9863-46bd-802d-cb02e6e064ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/29 19:59:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"spark://spark:7077\") \\\n",
    "                    .appName(\"Ass4-Q3\") \\\n",
    "                    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5bd4833-077b-4758-ad59-8b3b0996433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tempfile\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "def make_test_datasets(path, batch_size=64):\n",
    "    X = []\n",
    "    y = []\n",
    "    #convert = lambda category : int(category == 'dog')\n",
    "    \n",
    "    for p in os.listdir(path):\n",
    "        #category = p.split(\".\")[0]\n",
    "        #category = convert(category)\n",
    "        img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n",
    "        new_img_array = cv2.resize(img_array, dsize=(80,80)) / 255\n",
    "        X.append(new_img_array)\n",
    "        #y.append(category)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        tf.cast(X, tf.float32),\n",
    "        #tf.cast(y, tf.int64)\n",
    "        )\n",
    "    )\n",
    "    return dataset\n",
    "    \n",
    "\n",
    "def build_and_compile_cnn_model():\n",
    "    model = Sequential()\n",
    "    # Adds a densely-connected layer with 64 units to the model:\n",
    "    model.add(Conv2D(64,(3,3), activation = 'relu', input_shape = (80,80,1)))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    # Add another:\n",
    "    model.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    # Add a softmax layer with 10 output units:\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd99a18a-db31-44df-9a7e-1b412834adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Doing CPU training...\n",
      "Will run with 2 Spark tasks.\n",
      "Distributed training in progress...\n",
      "View Spark executor stderr logs to inspect training...\n",
      "Training with 2 slots is complete!                                              \n"
     ]
    }
   ],
   "source": [
    "def train(batch_size=64):\n",
    "    import cv2\n",
    "    import tempfile\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D\n",
    "    \n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "\n",
    "    path=\"/mnt/data_file/train_data/train\"\n",
    "    checkpoint_path = \"/mnt/data_file/ass4.model/\"\n",
    "    random_path = tempfile.TemporaryDirectory()\n",
    "\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "    chief_callback = ModelCheckpoint(checkpoint_path, \n",
    "                                     monitor='accuracy', \n",
    "                                     save_best_only=True,\n",
    "                                     mode='max')\n",
    "    dummy_callback = ModelCheckpoint(random_path.name, monitor='accuracy', save_best_only=True)\n",
    "\n",
    "    def make_datasets():\n",
    "        X = []\n",
    "        y = []\n",
    "        convert = lambda category : int(category == 'dog')\n",
    "        \n",
    "        for p in os.listdir(path):\n",
    "            category = p.split(\".\")[0]\n",
    "            category = convert(category)\n",
    "            img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n",
    "            new_img_array = cv2.resize(img_array, dsize=(80,80)) / 255\n",
    "            X.append(new_img_array)\n",
    "            y.append(category)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            tf.cast(X, tf.float32),\n",
    "            tf.cast(y, tf.int64))\n",
    "        )\n",
    "        dataset = dataset.repeat().shuffle(BUFFER_SIZE).batch(batch_size)\n",
    "        print(f\"Data size: {len(y)}\")\n",
    "        print(f\"Dog pictures #: {sum(y)}\")\n",
    "        return dataset\n",
    "        \n",
    "\n",
    "    def build_and_compile_cnn_model():\n",
    "        model = Sequential()\n",
    "        # Adds a densely-connected layer with 64 units to the model:\n",
    "        model.add(Conv2D(64,(3,3), activation = 'relu', input_shape = (80,80,1)))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "        # Add another:\n",
    "        model.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        # Add a softmax layer with 10 output units:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.compile(optimizer=\"adam\",\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    train_datasets = make_datasets()\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    train_datasets = train_datasets.with_options(options)\n",
    "    multi_worker_model = build_and_compile_cnn_model()\n",
    "    if 'TF_CONFIG' in os.environ:    \n",
    "        tf_config = json.loads(os.environ['TF_CONFIG'])    \n",
    "        node_index = tf_config['task']['index']    \n",
    "        is_chief = node_index == 0    \n",
    "        print(f\"Node Index: {node_index}, Is Chief: {is_chief}\")\n",
    "    callback = [chief_callback if is_chief else dummy_callback]\n",
    "    multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5, callbacks=callback)\n",
    "    print(max(multi_worker_model.predict(train_datasets, steps=5)))\n",
    "    random_path.cleanup()\n",
    "\n",
    "from spark_tensorflow_distributor import MirroredStrategyRunner\n",
    " \n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "runner = MirroredStrategyRunner(num_slots=2, local_mode=False, use_gpu=False)\n",
    "runner.run(train, batch_size=BATCH_SIZE_PER_REPLICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0839994c-5ec0-4669-bd3b-6837ba1b03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/mnt/data_file/ass4.model/\"\n",
    "test_path=\"/mnt/data_file/train_data/test1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d4cb13c-09f5-435a-8010-ae781dd4d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_and_compile_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d51bebb-e6db-4bdd-b26c-065aee4299b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c4a0d10-ed41-4e12-816e-371004d18a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 78, 78, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 39, 39, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 37, 37, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 18, 18, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20736)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1327168   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1364801 (5.21 MB)\n",
      "Trainable params: 1364801 (5.21 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51a7bc8c-1c64-4f1d-bf8b-4dc2143bf7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = make_test_datasets(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b70ad6cb-20d6-4c5b-89a6-b471e44cab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 6s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(test_data.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32b7b77b-7480-4364-aa51-9d1af11e00e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (predict > 0.5).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a70d3791-cc6d-41ee-b077-480010990c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa6338b9-ce23-4fc7-9fac-308238b39e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(80, 80), dtype=float32, numpy=\n",
      "array([[0.23921569, 0.21960784, 0.23529412, ..., 0.4       , 0.42745098,\n",
      "        0.4       ],\n",
      "       [0.21568628, 0.23529412, 0.24705882, ..., 0.38431373, 0.41960785,\n",
      "        0.3764706 ],\n",
      "       [0.19215687, 0.22745098, 0.22745098, ..., 0.3647059 , 0.39607844,\n",
      "        0.36078432],\n",
      "       ...,\n",
      "       [0.7647059 , 0.7372549 , 0.74509805, ..., 0.42352942, 0.47843137,\n",
      "        0.49411765],\n",
      "       [0.7490196 , 0.7647059 , 0.7647059 , ..., 0.79607844, 0.60784316,\n",
      "        0.45490196],\n",
      "       [0.7019608 , 0.7372549 , 0.74509805, ..., 0.5058824 , 0.7058824 ,\n",
      "        0.34117648]], dtype=float32)>,)\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b1266fd-1dd3-45a0-af98-c4955f1ac80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8ef399b-2c7f-407b-986c-3b4b12c88012",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "id_line = []\n",
    "def create_test1_data(test_path):\n",
    "    for p in os.listdir(test_path):\n",
    "        id_line.append(p.split(\".\")[0])\n",
    "        img_array = cv2.imread(os.path.join(test_path,p),cv2.IMREAD_GRAYSCALE)\n",
    "        new_img_array = cv2.resize(img_array, dsize=(80, 80))\n",
    "        X_test.append(new_img_array)\n",
    "create_test1_data(test_path)\n",
    "X_test = np.array(X_test).reshape(-1,80,80,1)\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b3f0c41-16d3-46fd-adaf-507841398bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09981bc1-74d0-457b-9357-b0aa866b498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_val = [int(round(p[0])) for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7b6354a-1587-47bd-b6de-252f28189af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(predicted_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
