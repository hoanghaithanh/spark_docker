{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02633f4b-acda-4631-adbb-f9851faa5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ea8e24-9863-46bd-802d-cb02e6e064ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 01:25:50 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "24/12/09 01:25:50 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "24/12/09 01:25:50 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"spark://spark-master:7077\") \\\n",
    "                    .appName(\"Ass4-Q3\") \\\n",
    "                    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5bd4833-077b-4758-ad59-8b3b0996433f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 01:25:56.705670: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-09 01:25:57.026948: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-09 01:25:58.104430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tempfile\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "def make_test_datasets(path, batch_size=64):\n",
    "    X = []\n",
    "    y = []\n",
    "    #convert = lambda category : int(category == 'dog')\n",
    "    \n",
    "    for p in os.listdir(path):\n",
    "        #category = p.split(\".\")[0]\n",
    "        #category = convert(category)\n",
    "        img_array = cv2.imread(os.path.join(path,p), cv2.IMREAD_COLOR)\n",
    "        new_img_array = cv2.resize(img_array, dsize=(150,150,3)) / 255\n",
    "        X.append(new_img_array)\n",
    "        #y.append(category)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        tf.cast(X, tf.float32),\n",
    "        #tf.cast(y, tf.int64)\n",
    "        )\n",
    "    )\n",
    "    return dataset\n",
    "    \n",
    "\n",
    "def build_and_compile_cnn_model():\n",
    "    model = Sequential()\n",
    "    # Adds a densely-connected layer with 64 units to the model:\n",
    "    model.add(Conv2D(64,(3,3), activation = 'relu', input_shape = (80,80,3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd99a18a-db31-44df-9a7e-1b412834adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size=64):\n",
    "    import cv2\n",
    "    import tempfile\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D\n",
    "    \n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "\n",
    "    path=\"/mnt/data_file/train_data/train\"\n",
    "    checkpoint_path = \"/mnt/data_file/ass4.model/\"\n",
    "    random_path = tempfile.TemporaryDirectory()\n",
    "\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "    chief_callback = ModelCheckpoint(checkpoint_path, \n",
    "                                     monitor='binary_accuracy', \n",
    "                                     save_best_only=True,\n",
    "                                     mode='max')\n",
    "    dummy_callback = ModelCheckpoint(random_path.name, monitor='binary_accuracy', save_best_only=True)\n",
    "\n",
    "    def make_datasets():\n",
    "        X = []\n",
    "        y = []\n",
    "        convert = lambda category : int(category == 'dog')\n",
    "        \n",
    "        for p in os.listdir(path):\n",
    "            category = p.split(\".\")[0]\n",
    "            category = convert(category)\n",
    "            img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n",
    "            new_img_array = cv2.resize(img_array, dsize=(80,80)) / 255.0\n",
    "            X.append(new_img_array)\n",
    "            y.append(category)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            tf.cast(X, tf.float32),\n",
    "            tf.cast(y, tf.int64))\n",
    "        )\n",
    "        dataset = dataset.repeat().shuffle(BUFFER_SIZE).batch(batch_size)\n",
    "        print(f\"Data size: {len(y)}\")\n",
    "        print(f\"Dog pictures #: {sum(y)}\")\n",
    "        return dataset\n",
    "        \n",
    "\n",
    "    def build_and_compile_cnn_model():\n",
    "        model = Sequential()\n",
    "        # Adds a densely-connected layer with 64 units to the model:\n",
    "        model.add(Conv2D(64,(3,3), activation = 'relu', input_shape = (80,80,1)))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "        # Add another:\n",
    "        model.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        # Add a softmax layer with 10 output units:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.compile(optimizer=\"adam\",\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['binary_accuracy'])\n",
    "        return model\n",
    "\n",
    "    train_datasets = make_datasets()\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    train_datasets = train_datasets.with_options(options)\n",
    "    multi_worker_model = build_and_compile_cnn_model()\n",
    "    if 'TF_CONFIG' in os.environ:    \n",
    "        tf_config = json.loads(os.environ['TF_CONFIG'])    \n",
    "        node_index = tf_config['task']['index']    \n",
    "        is_chief = node_index == 0    \n",
    "        print(f\"Node Index: {node_index}, Is Chief: {is_chief}\")\n",
    "    #callback = [chief_callback if is_chief else dummy_callback]\n",
    "    callback = chief_callback\n",
    "    multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5, callbacks=callback)\n",
    "    print(max(multi_worker_model.predict(train_datasets, steps=5)))\n",
    "    random_path.cleanup()\n",
    "    return multi_worker_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a9343f-0db3-4180-9db7-c809b662e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MirroredStrategyRunner:Doing CPU training...\n",
      "INFO:MirroredStrategyRunner:Will run with 2 Spark tasks.\n",
      "INFO:MirroredStrategyRunner:Distributed training in progress...\n",
      "INFO:MirroredStrategyRunner:View Spark executor stderr logs to inspect training...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.NullPointerException\njava.lang.NullPointerException\n\tat org.apache.spark.api.python.PythonAccumulatorV2.<init>(PythonRDD.scala:706)\n\tat org.apache.spark.api.python.PythonAccumulatorV2.copyAndReset(PythonRDD.scala:726)\n\tat org.apache.spark.api.python.PythonAccumulatorV2.copyAndReset(PythonRDD.scala:698)\n\tat org.apache.spark.util.AccumulatorV2.writeReplace(AccumulatorV2.scala:171)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteReplace(ObjectStreamClass.java:1244)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1136)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1570)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1397)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1332)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1397)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1332)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.api.python.PythonAccumulatorV2.<init>(PythonRDD.scala:706)\n\tat org.apache.spark.api.python.PythonAccumulatorV2.copyAndReset(PythonRDD.scala:726)\n\tat org.apache.spark.api.python.PythonAccumulatorV2.copyAndReset(PythonRDD.scala:698)\n\tat org.apache.spark.util.AccumulatorV2.writeReplace(AccumulatorV2.scala:171)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteReplace(ObjectStreamClass.java:1244)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1136)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1570)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1397)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1332)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m runner \u001b[38;5;241m=\u001b[39m MirroredStrategyRunner(num_slots\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, local_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE_PER_REPLICA\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/spark_tensorflow_distributor/mirrored_strategy_runner.py:232\u001b[0m, in \u001b[0;36mMirroredStrategyRunner.run\u001b[0;34m(self, train_fn, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistributed training in progress...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mView Spark executor stderr logs to inspect training...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    229\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_tasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_tasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbarrier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_task_program\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m--> 232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_slots\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m slots is complete!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.NullPointerException\njava.lang.NullPointerException\n\tat org.apache.spark.api.python.PythonAccumulatorV2.<init>(PythonRDD.scala:706)\n\tat org.apache.spark.api.python.PythonAccumulatorV2.copyAndReset(PythonRDD.scala:726)\n\tat org.apache.spark.api.python.PythonAccumulatorV2.copyAndReset(PythonRDD.scala:698)\n\tat org.apache.spark.util.AccumulatorV2.writeReplace(AccumulatorV2.scala:171)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteReplace(ObjectStreamClass.java:1244)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1136)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1570)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1397)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1332)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1397)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1332)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.api.python.PythonAccumulatorV2.<init>(PythonRDD.scala:706)\n\tat org.apache.spark.api.python.PythonAccumulatorV2.copyAndReset(PythonRDD.scala:726)\n\tat org.apache.spark.api.python.PythonAccumulatorV2.copyAndReset(PythonRDD.scala:698)\n\tat org.apache.spark.util.AccumulatorV2.writeReplace(AccumulatorV2.scala:171)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteReplace(ObjectStreamClass.java:1244)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1136)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1570)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1397)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1332)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2991)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    }
   ],
   "source": [
    "from spark_tensorflow_distributor import MirroredStrategyRunner\n",
    " \n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "runner = MirroredStrategyRunner(num_slots=2, local_mode=False, use_gpu=False)\n",
    "runner.run(train, batch_size=BATCH_SIZE_PER_REPLICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0839994c-5ec0-4669-bd3b-6837ba1b03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/mnt/data_file/ass4.model/\"\n",
    "test_path=\"/mnt/data_file/train_data/test1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d4cb13c-09f5-435a-8010-ae781dd4d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_and_compile_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d51bebb-e6db-4bdd-b26c-065aee4299b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c4a0d10-ed41-4e12-816e-371004d18a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 78, 78, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 39, 39, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 37, 37, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 18, 18, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20736)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1327168   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1364801 (5.21 MB)\n",
      "Trainable params: 1364801 (5.21 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51a7bc8c-1c64-4f1d-bf8b-4dc2143bf7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = make_test_datasets(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fec85c41-9566-4af5-8c03-46a4b79de1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=(TensorSpec(shape=(80, 80), dtype=tf.float32, name=None),)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b70ad6cb-20d6-4c5b-89a6-b471e44cab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 8s 35ms/step\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(test_data.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "32b7b77b-7480-4364-aa51-9d1af11e00e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (predict > 0.5).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a70d3791-cc6d-41ee-b077-480010990c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12500, 1), dtype=float32, numpy=\n",
       "array([[0.5],\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       ...,\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       [0.5]], dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.sigmoid(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa6338b9-ce23-4fc7-9fac-308238b39e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(80, 80), dtype=float32, numpy=\n",
      "array([[0.23921569, 0.21960784, 0.23529412, ..., 0.4       , 0.42745098,\n",
      "        0.4       ],\n",
      "       [0.21568628, 0.23529412, 0.24705882, ..., 0.38431373, 0.41960785,\n",
      "        0.3764706 ],\n",
      "       [0.19215687, 0.22745098, 0.22745098, ..., 0.3647059 , 0.39607844,\n",
      "        0.36078432],\n",
      "       ...,\n",
      "       [0.7647059 , 0.7372549 , 0.74509805, ..., 0.42352942, 0.47843137,\n",
      "        0.49411765],\n",
      "       [0.7490196 , 0.7647059 , 0.7647059 , ..., 0.79607844, 0.60784316,\n",
      "        0.45490196],\n",
      "       [0.7019608 , 0.7372549 , 0.74509805, ..., 0.5058824 , 0.7058824 ,\n",
      "        0.34117648]], dtype=float32)>,)\n"
     ]
    }
   ],
   "source": [
    "model.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b1266fd-1dd3-45a0-af98-c4955f1ac80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8ef399b-2c7f-407b-986c-3b4b12c88012",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "id_line = []\n",
    "def create_test1_data(test_path):\n",
    "    for p in os.listdir(test_path):\n",
    "        id_line.append(p.split(\".\")[0])\n",
    "        img_array = cv2.imread(os.path.join(test_path,p),cv2.IMREAD_GRAYSCALE)\n",
    "        new_img_array = cv2.resize(img_array, dsize=(80, 80))\n",
    "        X_test.append(new_img_array)\n",
    "create_test1_data(test_path)\n",
    "X_test = np.array(X_test).reshape(-1,80,80,1)\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b3f0c41-16d3-46fd-adaf-507841398bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09981bc1-74d0-457b-9357-b0aa866b498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_val = [int(round(p[0])) for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7b6354a-1587-47bd-b6de-252f28189af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(predicted_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "35d4c1aa-e450-41e9-9f3b-5aedd1b0907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(y_pred, thresh=0.5):\n",
    "  # Return a tensor with  `1` if `y_pred` > `0.5`, and `0` otherwise\n",
    "  return tf.cast(y_pred > thresh, tf.float32)\n",
    "\n",
    "def accuracy(y_pred, y):\n",
    "  # Return the proportion of matches between `y_pred` and `y`\n",
    "  y_pred = tf.math.sigmoid(y_pred)\n",
    "  y_pred_class = predict_class(y_pred)\n",
    "  check_equal = tf.cast(y_pred_class == y,tf.float32)\n",
    "  acc_val = tf.reduce_mean(check_equal)\n",
    "  return acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d45ed34d-c9a2-4805-b951-1e73bffe53de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 25000\n",
      "Dog pictures #: 12500\n",
      "Epoch 1/3\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.1434 - binary_accuracy: 1.0000INFO:tensorflow:Assets written to: /mnt/data_file/ass4.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /mnt/data_file/ass4.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 283ms/step - loss: 0.1434 - binary_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 2.1276e-09 - binary_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 9.0441e-13 - binary_accuracy: 1.0000\n",
      "5/5 [==============================] - 0s 35ms/step\n",
      "[9.635336e-13]\n"
     ]
    }
   ],
   "source": [
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5cf505b-f98a-41ad-863d-8f79e011fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets():\n",
    "    X = []\n",
    "    y = []\n",
    "    convert = lambda category : int(category == 'dog')\n",
    "    train_path = \"/mnt/data_file/train_data/train\"\n",
    "    for p in os.listdir(train_path):\n",
    "        category = p.split(\".\")[0]\n",
    "        category = convert(category)\n",
    "        img_array = cv2.imread(os.path.join(train_path,p),cv2.IMREAD_COLOR)\n",
    "        new_img_array = cv2.resize(img_array, dsize=(150,150)) / 255.0\n",
    "        X.append(new_img_array)\n",
    "        y.append(category)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        tf.cast(X, tf.float32),\n",
    "        tf.cast(y, tf.int64))\n",
    "    )\n",
    "    #dataset = dataset.repeat().shuffle(10000).batch(64)\n",
    "    print(f\"Data size: {len(y)}\")\n",
    "    print(f\"Dog pictures #: {sum(y)}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd65eb-37f2-4e4d-8367-668ff27d0708",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff31d9-66af-49cd-9a85-3124b67b85b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save(\"/mnt/data_file/train_data/saved_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5dc83-88f9-4fad-8c0c-fd9818ec6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_and_compile_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa709b5-f5fe-4252-82a9-5ad3edb6cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "model.fit(train_dataset.batch(64).shuffle(10000), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "593dbbb6-52ec-48e0-ae4a-68f0d95ffdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 20s 102ms/step - loss: 4.7014 - binary_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.7014031410217285, 0.5]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_dataset.batch(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f538b8b-d96c-4de3-9f8d-87b27a8128ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ef285-7f08-4c15-971c-dcd54edb62d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
